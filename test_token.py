from openai import OpenAI
import tiktoken

client = OpenAI(api_key="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")

messages = [
    {"role": "system", "content": "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù‡Ø³ØªÛŒ."},
    {"role": "user", "content": "Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø´Ø¹Ø± Ú©ÙˆØªØ§Ù‡ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø®ÙˆØ±Ø´ÛŒØ¯ Ø¨Ú¯Ùˆ."}
]

model_name = "gpt-4o"

# 1ï¸âƒ£ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ tiktoken
encoding = tiktoken.encoding_for_model(model_name)

def num_tokens_from_messages(messages, encoding):
    tokens_per_message = 3
    tokens_per_name = 1
    num_tokens = 0
    for msg in messages:
        num_tokens += tokens_per_message
        for key, value in msg.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3
    return num_tokens

predicted_input_tokens = num_tokens_from_messages(messages, encoding)
print(f"ğŸ“Œ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ tiktoken (ÙˆØ±ÙˆØ¯ÛŒ): {predicted_input_tokens}")

# 2ï¸âƒ£ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ù‡ OpenAI
response = client.chat.completions.create(
    model=model_name,
    messages=messages
)

prompt_tokens = response.usage.prompt_tokens
completion_tokens = response.usage.completion_tokens
total_tokens = response.usage.total_tokens

print(f"ğŸ“ Ú¯Ø²Ø§Ø±Ø´ OpenAI:")
print(f"   ÙˆØ±ÙˆØ¯ÛŒ: {prompt_tokens}")
print(f"   Ø®Ø±ÙˆØ¬ÛŒ: {completion_tokens}")
print(f"   Ù…Ø¬Ù…ÙˆØ¹: {total_tokens}")

# 3ï¸âƒ£ Ù…Ù‚Ø§ÛŒØ³Ù‡
diff_input = prompt_tokens - predicted_input_tokens
print(f"ğŸ“Š Ø§Ø®ØªÙ„Ø§Ù Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ ÙˆØ§Ù‚Ø¹ÛŒ: {diff_input} ØªÙˆÚ©Ù†")

# Ú†Ø§Ù¾ Ù¾Ø§Ø³Ø® (Ø¯Ø± ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„ ÙØ§Ø±Ø³ÛŒ Ø¯Ø± ØªØ±Ù…ÛŒÙ†Ø§Ù„)
with open("output.txt", "w", encoding="utf-8") as f:
    f.write(response.choices[0].message.content)
print("âœ… Ù¾Ø§Ø³Ø® Ø¯Ø± output.txt Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
