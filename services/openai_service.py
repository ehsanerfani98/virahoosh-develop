# services/openai_service.py

from openai import OpenAI, AsyncOpenAI
from core.config import OPENAI_API_KEY, DEEPSEEK_API_KEY
from PIL import Image
from io import BytesIO
import asyncio, base64
from fastapi import WebSocket, WebSocketDisconnect
import logging
from fastapi.websockets import WebSocketState
import time
import io
import wave
import json

# Configure detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AudioSession:
    """Track audio session state"""
    def __init__(self):
        self.audio_buffer_has_data = False
        self.total_audio_sent = 0
        self.total_audio_received = 0
        self.session_start = time.time()
        self.last_audio_time = None
        self.openai_connected = False
        self.websocket_connected = False

    def log_stats(self):
        duration = time.time() - self.session_start
        logger.info(f"Session stats: {duration:.1f}s, sent: {self.total_audio_sent} bytes, received: {self.total_audio_received} bytes")
        
        
clientb = AsyncOpenAI(api_key=OPENAI_API_KEY)

client = OpenAI(api_key=OPENAI_API_KEY)


def run_openai_prompt(prompt_text: str, system_prompt: str, max_tokens: int, model: str, provider: str) -> str:
    try:
        if provider == "openai":
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt_text}
                ],
                temperature=0.5,
                max_tokens=max_tokens
            )
            return {
                'error': False,
                'message' : response.choices[0].message.content.strip(),
                'total_tokens' : response.usage.total_tokens
            }

        elif provider == "deepseek":
            import requests
            deepseek_url = "https://api.deepseek.com/chat/completions"
            headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}"}
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt_text}
                ],
            }
            r = requests.post(deepseek_url, json=payload, headers=headers)
            r.raise_for_status()
            return {
                'error': False,
                'message': r.json()["choices"][0]["message"]["content"].strip()
            }
        else:
            return {
                'error': True,
                'message': "Ø³Ø±ÙˆÛŒØ³ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø§Ø³Øª"
            }

    except Exception as e:
        return {
            'error': True,
            'message': f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ: {e}"
        }


def generate_image_by_prompt(prompt: str, size: str = "1792x1024") -> str:
    try:
        response = client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            n=1,
            size=size,
            response_format="url"
        )
        return response.data[0].url
    except Exception as e:
        return e


def translate_to_english(text: str, model: str, provider: str, max_tokens: int) -> str:
    try:
        if provider == "openai":
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an English translator.Just translate, do not add any extra explanation."},
                    {"role": "user", "content": f"Translate the text '{text}' into English."}
                ],
                temperature=0,
                max_tokens=max_tokens
            )
            return {
                'error': False,
                'message' : response.choices[0].message.content.strip(),
                'total_tokens' : response.usage.total_tokens
            }
        elif provider == "deepseek":
            # Ù†Ù…ÙˆÙ†Ù‡ Ø³Ø§Ø®ØªÚ¯ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ù‚Ø¨Ù„ØŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø§ API ÙˆØ§Ù‚Ø¹ÛŒ DeepSeek
            import requests
            deepseek_url = "https://api.deepseek.com/chat/completions"
            headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}"}
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": "You are an English translator."},
                    {"role": "user", "content": f"Translate the text '{text}' into English.Just translate, do not add any extra explanation."}
                ],
            }
            r = requests.post(deepseek_url, json=payload, headers=headers)
            r.raise_for_status()
            return r.json()["choices"][0]["message"]["content"].strip()

        else:
            return {
                'error': True,
                'message': "Ø³Ø±ÙˆÛŒØ³ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø§Ø³Øª"
            }
    except Exception as e:
        return {
            'error': True,
            'message': f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ: {e}"
        }

def text_to_speech(
    input_text: str,
    model: str,
    voice: str,
    output_path: str,
    format: str,
    instructions: str = "Ø¨Ø§ Ù„Ø­Ù†ÛŒ Ø¹ØµØ¨Ø§Ù†ÛŒ Ùˆ Ø®Ø´Ù† ØµØ­Ø¨Øª Ú©Ù†"
) -> str:
    try:
        response = client.audio.speech.create(
            model=model,
            input=input_text,
            voice=voice,
            response_format=format,
            instructions=instructions,
        )
        # Ø°Ø®ÛŒØ±Ù‡ ØµØ¯Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² response.content)
        with open(output_path, "wb") as f:
            f.write(response.content)
        return output_path

    except Exception as e:
        return f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§: {e}"


def analyze_image_with_openai_vision(image: Image.Image, max_tokens: int, prompt: str, model: str = "gpt-4-vision-preview") -> str:
    try:
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {
                            "url": f"data:image/png;base64,{img_str}"
                        }}
                    ]
                }
            ],
            max_completion_tokens=max_tokens
        )
        return {
            'error': False,
            'message' : response.choices[0].message.content.strip(),
            'total_tokens' : response.usage.total_tokens
        }
    except Exception as e:
        return {
            'error': True,
            'message': f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ ØªØµÙˆÛŒØ±: {e}"
        }


# async def realtime_audio_relay(websocket: WebSocket, deployment: str):
#     session = AudioSession()
#     session.websocket_connected = True

#     try:
#         logger.info(f"Connecting to OpenAI realtime API with model: {deployment}")

#         async with clientb.beta.realtime.connect(model=deployment) as conn:
#             session.openai_connected = True
#             logger.info("âœ… Connected to OpenAI realtime API")

#             session_config = {
#                 "modalities": ["text", "audio"],
#                 "instructions": "You are a helpful assistant. Respond naturally and conversationally.",
#                 "voice": "alloy",
#                 "input_audio_format": "webm/opus",
#                 "output_audio_format": "pcm16",
#                 "input_audio_transcription": {
#                     "model": "whisper-1"
#                 },
#                 "turn_detection": {
#                     "type": "server_vad",
#                     "threshold": 0.5,
#                     "prefix_padding_ms": 300,
#                     "silence_duration_ms": 500
#                 },
#                 "tool_choice": "auto",
#                 "temperature": 0.8,
#                 "max_response_output_tokens": 4096
#             }

#             logger.info("Updating session configuration...")
#             await conn.session.update(session=session_config)
#             logger.info("âœ… Session configuration updated")

#             async def send_to_openai():
#                 logger.info("ðŸŽ¤ Audio receiver task started")

#                 while session.websocket_connected:
#                     try:
#                         msg = await websocket.receive_bytes()
#                         session.last_audio_time = time.time()
#                         session.total_audio_received += len(msg)
#                         logger.info(f"ðŸ“¥ Received audio: {len(msg)} bytes (total: {session.total_audio_received})")

#                         # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„ Ø¯Ø§Ø¯Ù‡
#                         if len(msg) < 480:  # Ú©Ù…ØªØ± Ø§Ø² 20ms Ø¯Ø± 24kHz*2byte
#                             logger.warning("âš ï¸ Audio chunk too short - skipping")
#                             continue

#                         audio64 = base64.b64encode(msg).decode()
#                         await conn.send({"type": "input_audio_buffer.append", "audio": audio64})

#                         session.audio_buffer_has_data = True
#                         session.total_audio_sent += len(msg)
#                         logger.info(f"âœ… Audio sent to OpenAI (total sent: {session.total_audio_sent} bytes)")

#                     except WebSocketDisconnect:
#                         logger.info("ðŸ”Œ Client disconnected")
#                         session.websocket_connected = False
#                         break
#                     except Exception as e:
#                         logger.error(f"âŒ Error in audio receiver: {e}")
#                         break

#                 if session.audio_buffer_has_data:
#                     logger.info("ðŸ’¾ Committing audio buffer to OpenAI...")
#                     try:
#                         await conn.send({"type": "input_audio_buffer.commit"})
#                         logger.info("âœ… Audio buffer committed successfully")
#                     except Exception as e:
#                         logger.error(f"âŒ Error committing audio buffer: {e}")
#                 else:
#                     logger.info("â„¹ï¸ No audio data to commit")

#                 logger.info("ðŸ Audio sender task finished")

#             async def recv_from_openai():
#                 logger.info("ðŸŽ§ OpenAI receiver task started")

#                 async for ev in conn:
#                     if not session.websocket_connected:
#                         logger.info("ðŸ”Œ WebSocket disconnected, stopping OpenAI receiver")
#                         break
#                     if websocket.client_state == WebSocketState.DISCONNECTED:
#                         logger.info("ðŸ”Œ WebSocket client disconnected, stopping OpenAI receiver")
#                         break

#                     try:
#                         logger.debug(f"ðŸ“¨ Received OpenAI event: {ev.type}")

#                         if ev.type == 'error':
#                             logger.error(f"âŒ OpenAI error: {ev.error}")
#                             await websocket.send_json({"type": "error", "error": str(ev.error)})

#                         elif ev.type == "response.audio.delta":
#                             if hasattr(ev, 'delta') and ev.delta:
#                                 audio_data = base64.b64decode(ev.delta)
#                                 await websocket.send_bytes(audio_data)

#                         elif ev.type == "response.audio.done":
#                             await websocket.send_json({"type": "audio_done"})

#                         elif ev.type == "input_audio_buffer.speech_started":
#                             await websocket.send_json({"type": "status", "status": "listening"})

#                         elif ev.type == "input_audio_buffer.speech_stopped":
#                             await websocket.send_json({"type": "status", "status": "processing"})

#                         elif ev.type == "conversation.item.input_audio_transcription.completed":
#                             transcript = getattr(ev, "transcript", "")
#                             if transcript:
#                                 await websocket.send_json({"type": "transcript", "transcript": transcript})

#                         elif ev.type == "response.text.delta":
#                             if hasattr(ev, 'delta') and ev.delta:
#                                 await websocket.send_json({"type": "text", "text": ev.delta})

#                         elif ev.type == "session.created":
#                             await websocket.send_json({"type": "session_ready", "message": "Ready to receive audio"})

#                         elif ev.type == "session.updated":
#                             pass

#                     except Exception as e:
#                         if session.websocket_connected:
#                             logger.error(f"âŒ Error processing OpenAI event: {e}")
#                         break

#                 logger.info("ðŸ OpenAI receiver task finished")

#             logger.info("ðŸš€ Starting audio relay tasks...")
#             await asyncio.gather(send_to_openai(), recv_from_openai())

#     except Exception as e:
#         logger.error(f"âŒ Fatal error in realtime_audio_relay: {e}")
#         if session.websocket_connected:
#             try:
#                 await websocket.send_json({"type": "error", "error": "Server connection error"})
#             except: pass
#     finally:
#         session.websocket_connected = False
#         session.openai_connected = False
#         session.log_stats()
#         logger.info("ðŸ Audio relay finished")


def convert_audio_to_pcm16_detailed(raw_bytes: bytes) -> bytes:
    try:
        with wave.open(io.BytesIO(raw_bytes), 'rb') as wav_file:
            n_channels = wav_file.getnchannels()
            sampwidth = wav_file.getsampwidth()
            framerate = wav_file.getframerate()
            n_frames = wav_file.getnframes()
            pcm_data = wav_file.readframes(n_frames)

            logger.info(f"WAV Info: {n_channels}ch, {sampwidth*8}-bit, {framerate}Hz, {n_frames} frames")

            if sampwidth != 2:
                logger.warning(f"âš ï¸ Unexpected sample width: {sampwidth*8} bits (expected 16-bit)")
            if n_channels != 1:
                logger.warning(f"âš ï¸ Unexpected number of channels: {n_channels} (expected mono)")
            if framerate != 24000:
                logger.warning(f"âš ï¸ Unexpected sample rate: {framerate} Hz (expected 24000 Hz)")

            return pcm_data
    except Exception as e:
        logger.error(f"âŒ Error decoding WAV/PCM audio: {e}")
        return b''


# ===============================================================================================================================

async def realtime_audio_relay(websocket: WebSocket, deployment: str = "whisper-1"):
    try:
        await websocket.send_json({
            "type": "session_ready",
            "message": "Ready to receive audio"
        })

        while True:
            message = await websocket.receive()

            if message["type"] == "websocket.disconnect":
                break

            if message["type"] == "websocket.receive":
                if "text" in message:
                    data = json.loads(message["text"])
                    if data.get("type") == "audio_end":
                        # Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø¨Ø¹Ø¯Ø§Ù‹ Ú©Ø§Ø±ÛŒ Ù…Ø«Ù„ Ù¾Ø§ÛŒØ§Ù† Ø³Ø´Ù† Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒ
                        continue

                elif "bytes" in message:
                    audio_bytes = message["bytes"]

                    # ØµØ¯Ø§ Ø±Ùˆ Ø¨Ù‡ Whisper Ø¨Ø¯Ù‡
                    audio_file = io.BytesIO(audio_bytes)
                    audio_file.name = "chunk.webm"  # Ø®ÛŒÙ„ÛŒ Ù…Ù‡Ù…Ù‡ Ú©Ù‡ Ù¾Ø³ÙˆÙ†Ø¯ webm Ø¨Ø§Ø´Ù‡

                    try:
                        transcript = await client.audio.transcriptions.create(
                            model=deployment,
                            file=audio_file,
                            language="fa"
                        )
                        await websocket.send_json({
                            "type": "transcript",
                            "transcript": transcript.text
                        })

                    except Exception as e:
                        await websocket.send_json({
                            "type": "error",
                            "error": f"Whisper error: {str(e)}"
                        })

    except WebSocketDisconnect:
        pass
    except Exception as e:
        await websocket.send_json({"type": "error", "error": str(e)})
    finally:
        await websocket.close()



def text_to_speech_stream(
    input_text: str,
    model: str,
    voice: str,
    format: str,
    instructions: str = "Speak in a calm and very clear, measured tone"
) -> bytes:
    try:
        response = client.audio.speech.create(
            model=model,
            input=input_text,
            voice=voice,
            response_format=format,
            instructions=instructions,
        )
        return response.content

    except Exception as e:
        raise Exception(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§: {e}")
